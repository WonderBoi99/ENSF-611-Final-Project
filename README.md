# ML-Project

**About:**<br>
Implemented decision trees, random forest and gradient boosting models to the Bikeshare data and compared their performances with their default parameters. Then, played around with some of their parameters which like max_depth, max_leaf_nodes, n_estimators, max_features, learning rate. Took best model and did more analysis with the best possible model.<br>
Bikeshare dataset: https://www.scikit- yb.org/en/latest/api/datasets/bikeshare.html<br>
Scikit-learn models: DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor

**Code is divided into sections:**
* Dataset Details
* Start of ML
* Testing Models with their default parameters
* Modifying Parameters for Decision Trees
* Modifying Parameters for Random Forests
* Modifying Parameters for Gradient Boosting
* More Analysis with Best Decision Tree model
* More Analysis with Best RandomForests model
* More Analysis with Best GradientBoosting model

**Results:**
* Best Descision Tree model when max_leaf_nodes=100 & max_depth=10
* Best Random Forest model when n_estimators=50 & max_depth=15
* Best Gradient Boosting model when learning_rate=0.6 & max_features=9

**Reflection:**
* Purpose: wanted to gain more confidence, I wanted to get more practise on tree ML algothrithms and understand better the impact on the ml model when tuning their parameters.
* Interperted the reasons for how modifying the hyperparameters impacted the model was the most challenging part for me. I found using the model and changing the hyperparametrs fairly easy. I now feel more comfortable with applying ML tree algorithms and feel like I also now have a better understanding on how they work.
